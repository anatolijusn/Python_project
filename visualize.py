import os
import sys
import findspark
import time
import json
from pyspark import SparkConf, SparkContext
from pyspark.sql import SparkSession
import os
import sys
import findspark
import time
from pyspark import SparkConf, SparkContext
from pyspark.sql import SparkSession
import operator
import functools
import plotnine as gg
import pyspark
import pandas as pd
import pyspark.sql.functions as sql_fn
from pyspark.ml import stat
from pyspark.ml import feature
import IPython

import pyspark.sql.functions as sql_fn
from pyspark.ml import stat
from pyspark.ml import feature
import uuid

def compute_corr(df, columns, method="pearson"):
    assembler = feature.VectorAssembler(inputCols=columns, 
        outputCol="featuresCorrelation")
    corr_featurized_df = assembler.transform(df)
    corr_df = stat.Correlation.corr(corr_featurized_df, "featuresCorrelation", method)
    corr_matrix = corr_df.first()[0].toArray()
    corr_pddf = pd.DataFrame(corr_matrix, columns=columns, index=columns)
    return corr_pddf
findspark.init()

spark = (SparkSession
    .builder
    .appName("ClusterPrj")
    .enableHiveSupport()
    .getOrCreate())
print("start visualization")
customer_usage = ""
customer_churn = ""
#parametrus skaitau is json failo cia jau po "sampling" gauti failai
json_file_path = "Params.json"
with open(json_file_path, 'r') as j:
     contents = json.load(j)

customer_usage = contents['customer_usage']
customer_churn = contents['customer_churn']
print(customer_usage)
print(customer_churn)
usage_df = spark.read.csv(customer_usage,header=True,inferSchema=True)
summary_usage_df = usage_df.describe()
# bandau gauti descritpion statistikos aprasyma ir irasyti ji csv
summary_usage_df.toPandas().transpose()
(summary_usage_df
    .toPandas()
    .transpose()
    .to_csv("summary__usage.csv", 
            index=True, header=False))
pd.options.display.max_columns = 200
#sio metu turime tik 2 stulpelius histogramai aprasyti. Turbut mazai. Reiketu dar prideti?
histogram_columns = ["calls_outgoing_to_onnet_spendings", 
    "sms_outgoing_spendings",]
columns_correlation = [
    c for c in usage_df.columns 
    if c not in {"user_account_id", "year"}
]
print(columns_correlation)

# irasau i csv faila koreliacijos rezultatus
corr_pearson_pddf = compute_corr(usage_df, columns_correlation)
corr_spearman_pddf = compute_corr(usage_df, columns_correlation, "spearman")
corr_pearson_pddf.to_csv("corr_pearson__usage.csv", index=True, header=True)
corr_spearman_pddf.to_csv("corr_spearman__usage.csv", index=True, header=True)
n_bins = 30
histogram_dataset = [
    (c, usage_df.select(c).rdd.map(lambda r: r[0]).histogram(n_bins),) 
    for c in histogram_columns
]

histogram_pddfs = [
    (column, pd.DataFrame.from_records(zip(xs, ys + [0]), columns=["x", "y"]))
    for column, (xs, ys) in histogram_dataset
]

# rasau histograma ir piesini i kataloga
for column, hist_df in histogram_pddfs:
    hist_df.to_csv("hist__{}.csv".format(column), index=False)
    plot_hist = (gg.ggplot(gg.aes(x="x", weight="y"), hist_df) + gg.geom_bar() + gg.xlab(column) + gg.ylab("da≈ænis") + gg.ggtitle("'{}' histograma".format(column)) + gg.theme_bw())

    plot_hist.save("hist__{}.png".format(column))




print("stop")
spark.stop()